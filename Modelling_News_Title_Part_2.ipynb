{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Modelling News Title - Part 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "259.797px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBZmnlk4ngql"
      },
      "source": [
        "Alert! This script needs to access your GDrive\n",
        "\n",
        "Please upload the files to 'drive/My Drive/Colab Notebooks'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu2m4qFynkTP",
        "outputId": "dae0a7ab-e9d1-4761-eaa3-1542b6a5f0ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BdPJ7bEoBJq",
        "outputId": "c8d1e62b-1a20-4713-a4de-f45fc3d37beb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "!pip install polyglot"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting polyglot\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/98/e24e2489114c5112b083714277204d92d372f5bbe00d5507acf40370edb9/polyglot-16.7.4.tar.gz (126kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 3.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: polyglot\n",
            "  Building wheel for polyglot (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for polyglot: filename=polyglot-16.7.4-py2.py3-none-any.whl size=52559 sha256=17db4dfa6f5442d7be0360e53296d5d11025af56795b6feee890397b5c702fa1\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/91/ef/f1369fdc1203b0a9347d4b24f149b83a305f39ab047986d9da\n",
            "Successfully built polyglot\n",
            "Installing collected packages: polyglot\n",
            "Successfully installed polyglot-16.7.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF7uF3nS5DUd"
      },
      "source": [
        "# Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-06T12:03:58.538936Z",
          "start_time": "2020-10-06T12:03:58.045071Z"
        },
        "id": "tWgc-Asj5DUe",
        "outputId": "999713ad-9c38-4de8-b0ba-f4f1f9d97541",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import numpy as np\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_validate\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "from polyglot.mapping import Embedding\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "pd.set_option('max_columns', 1000)\n",
        "pd.set_option('max_rows', 1000)\n",
        "\n",
        "punctuation = string.punctuation # list of punctuation\n",
        "digit = [i for i in range(0,10)] # list of digits\n",
        "\n",
        "english_stemmer = SnowballStemmer(\"english\", ignore_stopwords=True) # english stemmer\n",
        "en_stops = set(stopwords.words('english'))  # english stopwords\n",
        "nlp = spacy.load(\"en_core_web_sm\") # model to do lemmatization\n",
        "words, embeddings = pickle.load(open('drive/My Drive/Colab Notebooks/polyglot-en.pkl', 'rb'), encoding='latin1') # word embedding from polyglot\n",
        "\n",
        "# Special tokens\n",
        "Token_ID = {\"<UNK>\": 0, \"<S>\": 1, \"</S>\":2, \"<PAD>\": 3}\n",
        "ID_Token = {v:k for k,v in Token_ID.items()}\n",
        "\n",
        "# Map words to indices and vice versa\n",
        "word_id = {w:i for (i, w) in enumerate(words)}\n",
        "id_word = dict(enumerate(words))\n",
        "\n",
        "# Normalize digits by replacing them with #\n",
        "DIGITS = re.compile(\"[0-9]\", re.UNICODE)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQFNe9AL5DUh"
      },
      "source": [
        "# Data import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-06T12:04:01.628003Z",
          "start_time": "2020-10-06T12:04:00.505159Z"
        },
        "id": "ceH1ctpH5DUi",
        "outputId": "f9988f12-afd5-4590-9918-5b0784e3dac7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "df_data = pd.read_excel('./drive/My Drive/Colab Notebooks/News Title.xls')\n",
        "print(df_data.shape[0])\n",
        "df_data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65535\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>News Title</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Google+ rolls out 'Stories' for tricked out ph...</td>\n",
              "      <td>Technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Dov Charney's Redeeming Quality</td>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>White God adds Un Certain Regard to the Palm Dog</td>\n",
              "      <td>Entertainment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Google shows off Androids for wearables, cars,...</td>\n",
              "      <td>Technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>China May new bank loans at 870.8 bln yuan</td>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   No                                         News Title       Category\n",
              "0   1  Google+ rolls out 'Stories' for tricked out ph...     Technology\n",
              "1   2                    Dov Charney's Redeeming Quality       Business\n",
              "2   3   White God adds Un Certain Regard to the Palm Dog  Entertainment\n",
              "3   4  Google shows off Androids for wearables, cars,...     Technology\n",
              "4   5         China May new bank loans at 870.8 bln yuan       Business"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-06T12:04:04.471914Z",
          "start_time": "2020-10-06T12:04:04.447641Z"
        },
        "id": "1SmkfSoU5DUp",
        "outputId": "b6e7f48d-159b-42a6-c60b-9a444fdec459",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "df_data['Category'].value_counts(dropna=False)\n",
        "\n",
        "# the target class is imbalanced"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Entertainment    23961\n",
              "Business         17707\n",
              "Technology       16776\n",
              "Medical           7091\n",
              "Name: Category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kefqQdWS5DUs"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-06T12:04:06.248912Z",
          "start_time": "2020-10-06T12:04:06.228648Z"
        },
        "id": "_hssRBVU5DUs"
      },
      "source": [
        "'''\n",
        "Function to clean the data, which includes:\n",
        "1. Lowercasing\n",
        "2. Punctuation removal\n",
        "3. Digit removal\n",
        "''' \n",
        "def cleaning(text):\n",
        "    # lowercase\n",
        "    normal = text.lower()\n",
        "    # remove punctuation\n",
        "    normal = re.sub(r'[^\\w\\s]', '', normal) \n",
        "    # remove numbers\n",
        "    normal = re.sub(r'\\d+', ' ', normal)\n",
        "    return normal\n",
        "\n",
        "'''\n",
        "Function to normalize the form of the token (lemmatization)\n",
        "and to remove stopwords\n",
        "'''\n",
        "def normalize_and_remove_stopwords(text):\n",
        "    tokens = nlp(text)\n",
        "    token_new = []\n",
        "    \n",
        "    for k in tokens:\n",
        "        if k.lemma_ not in en_stops:\n",
        "            token_new.append(k.lemma_)\n",
        "\n",
        "    str_clean = ' '.join(token_new)\n",
        "    return str_clean\n",
        "\n",
        "'''\n",
        "Function to do stemming, in this case, we use lemmatization\n",
        "instead of stemming\n",
        "'''\n",
        "def stemming(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    stem_sentence = []\n",
        "    for k in tokens:\n",
        "        stem_word = english_stemmer.stem(k)\n",
        "        stem_sentence.append(stem_word)\n",
        "\n",
        "    stem_sentence_str = ' '.join(stem_sentence)\n",
        "    return stem_sentence_str\n",
        "\n",
        "'''\n",
        "Data preprocessing function, which includes:\n",
        "1. Text cleaning,\n",
        "2. Text normalization, and\n",
        "3. Stopword removal\n",
        "'''\n",
        "def preprocessing(list_text):\n",
        "    text_clean = []\n",
        "    for t in list_text:\n",
        "        normal = cleaning(t)\n",
        "#         normal = stemming(normal)\n",
        "        normal = normalize_and_remove_stopwords(normal)\n",
        "        text_clean.append(normal)\n",
        "    return text_clean"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-06T05:27:23.917328Z",
          "start_time": "2020-10-06T05:18:24.771949Z"
        },
        "id": "DDC7udnv5DUv",
        "outputId": "ddf16850-6d74-48c2-e46b-23b0991d105f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "raw_text = df_data['News Title']\n",
        "\n",
        "clean_text = preprocessing(raw_text) # do the preprocessing\n",
        "clean_text[:3]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['google roll story trick photo playback',\n",
              " 'dov charney redeem quality',\n",
              " 'white god add un certain regard palm dog']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-06T05:28:28.175841Z",
          "start_time": "2020-10-06T05:28:28.077284Z"
        },
        "id": "sj3OwUoh5DUz"
      },
      "source": [
        "# save the clean comments to csv, so we can use it later on\n",
        "df_clean_title = pd.DataFrame(clean_text, columns=['title'])\n",
        "df_clean_title.to_csv('drive/My Drive/Colab Notebooks/df_clean_title_no_stemming.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXVZZPq18q-8"
      },
      "source": [
        "df_clean_title = pd.read_csv('./drive/My Drive/Colab Notebooks/df_clean_title_no_stemming.csv')\n",
        "clean_title = df_clean_title['title'] # clean text"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7Z95YKJ5DU7"
      },
      "source": [
        "# Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-06T12:04:13.593122Z",
          "start_time": "2020-10-06T12:04:12.930113Z"
        },
        "id": "9B2THLuf5DU7",
        "outputId": "ace09bf9-d97d-4e97-a6e5-b63398277c9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "'''\n",
        "Function to extract TF (1-gram) features\n",
        "'''\n",
        "def tf_extraction(text, ngram_start, ngram_end):\n",
        "    ngram = CountVectorizer(ngram_range=(ngram_start, ngram_end), max_features=3000)\n",
        "    ngram_matrix = ngram.fit_transform(np.array(text)).todense()\n",
        "    return ngram_matrix\n",
        "\n",
        "# unigram features\n",
        "ngram_feat = tf_extraction(clean_title, 1, 1)\n",
        "print(ngram_feat[:3])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Sqqo4IU87fu"
      },
      "source": [
        "'''\n",
        "In case the word is not available in the vocabulary,\n",
        "we can try multiple case normalizing procedure.\n",
        "We consider the best substitute to be the one with the lowest index,\n",
        "which is equivalent to the most frequent alternative.\n",
        "\n",
        "Source: https://nbviewer.jupyter.org/gist/aboSamoor/6046170\n",
        "''' \n",
        "def case_normalizer(word, dictionary):\n",
        "    w = word\n",
        "    lower = (dictionary.get(w.lower(), 1e2), w.lower())\n",
        "    upper = (dictionary.get(w.upper(), 1e2), w.upper())\n",
        "    title = (dictionary.get(w.title(), 1e2), w.title())\n",
        "    results = [lower, upper, title]\n",
        "    results.sort()\n",
        "    index, w = results[0]\n",
        "    if index != 1e2:\n",
        "        return w\n",
        "    return word\n",
        "\n",
        "'''\n",
        "Find the closest alternative in case the word is OOV.\n",
        "\n",
        "Source: https://nbviewer.jupyter.org/gist/aboSamoor/6046170\n",
        "'''\n",
        "def normalize(word, word_id):\n",
        "    if not word in word_id:\n",
        "        word = DIGITS.sub(\"#\", word)\n",
        "    if not word in word_id:\n",
        "        word = case_normalizer(word, word_id)\n",
        "\n",
        "    if not word in word_id:\n",
        "        return None\n",
        "    return word"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-06T12:04:25.385975Z",
          "start_time": "2020-10-06T12:04:25.370092Z"
        },
        "id": "2YKqFu6c5DU-"
      },
      "source": [
        "'''\n",
        "Function to retrieve the Euclidean distance\n",
        "between two words in the embedding vectors \n",
        "'''\n",
        "def get_distance(word1, word2, embeddings, word_id, id_word):\n",
        "    word1 = normalize(word1, word_id)\n",
        "    word2 = normalize(word2, word_id)\n",
        "    if not word1 or not word2: # if word 1 or word 2 not found in the embedding vectors, return 100 as the distance score\n",
        "        return 1e2\n",
        "    word1_index = word_id[word1]\n",
        "    word2_index = word_id[word2]\n",
        "    e1 = embeddings[word1_index]\n",
        "    e2 = embeddings[word2_index]\n",
        "    distance = ((e2 - e1) ** 2).sum() ** 0.5\n",
        "    return distance"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-06T12:06:07.610604Z",
          "start_time": "2020-10-06T12:05:44.202973Z"
        },
        "id": "BvobSK8H5DVB",
        "outputId": "76c2057f-f59b-4f84-fe16-d4b54beb9707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "'''\n",
        "Function to extract the embedding features, which include minimum distance of text to:\n",
        "1. Entertainment word\n",
        "2. Technology word\n",
        "3. Medical word\n",
        "4. Business word\n",
        "'''\n",
        "def embedding_extraction(text):\n",
        "    all_embedding_feat = []\n",
        "    for t in text:\n",
        "        entertainment_distances = []\n",
        "        technology_distances = []\n",
        "        medical_distances = []\n",
        "        business_distances = []\n",
        "        token = nltk.word_tokenize(t)\n",
        "        for k in token:\n",
        "            entertainment_distances.append(get_distance(k, 'entertainment', embeddings, word_id, id_word))\n",
        "            technology_distances.append(get_distance(k, 'technology', embeddings, word_id, id_word))\n",
        "            medical_distances.append(get_distance(k, 'medical', embeddings, word_id, id_word))\n",
        "            business_distances.append(get_distance(k, 'business', embeddings, word_id, id_word))\n",
        "        all_embedding_feat.append([min(entertainment_distances), min(technology_distances), min(medical_distances), min(business_distances)])\n",
        "    return all_embedding_feat\n",
        "\n",
        "embed_feat = embedding_extraction(clean_title)\n",
        "embed_feat[:3]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2.913120148087336, 2.948760506100634, 3.5125143573508284, 3.091267832678769],\n",
              " [2.4571364339319977,\n",
              "  3.0249680383428195,\n",
              "  3.3134951266334514,\n",
              "  2.932971141591275],\n",
              " [3.041295033226138,\n",
              "  3.5203397205450035,\n",
              "  3.7982305924660396,\n",
              "  3.269813735043243]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lf3sjM4sLcl",
        "outputId": "96a11954-2b31-47e8-8448-1a0d39883b32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'''\n",
        "Function to extract orthography and url occurence features\n",
        "'''\n",
        "def ortography(text):\n",
        "    all_orto_feat = []\n",
        "    for t in text:\n",
        "        capital_count = sum(1 for c in t if c.isupper())\n",
        "        exclamation_count = sum(1 for c in t if c == \"!\")\n",
        "        punctuation_count = sum(1 for c in t if c in punctuation)\n",
        "        word_len = len(nltk.word_tokenize(t))\n",
        "        char_len = len(t)\n",
        "        digit_occurence = sum(1 for c in t if c in digit)\n",
        "        orto_feat = [capital_count, exclamation_count, punctuation_count, word_len, char_len, digit_occurence]\n",
        "        all_orto_feat.append(orto_feat)\n",
        "    return all_orto_feat\n",
        "\n",
        "orto_feat = ortography(df_data['News Title'])\n",
        "orto_feat[:3]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 0, 3, 10, 58, 0], [4, 0, 1, 5, 31, 0], [7, 0, 0, 10, 48, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-06T12:07:17.530517Z",
          "start_time": "2020-10-06T12:07:16.746239Z"
        },
        "id": "cdCIowsf5DVF",
        "outputId": "523623ae-b9b1-4ef3-ae7d-004c1d5ed875",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "'''\n",
        "Function to extract TF-IDF (1-gram) features\n",
        "'''\n",
        "def tf_idf_extraction(text):\n",
        "    vectorizer = TfidfVectorizer(max_features=3000)\n",
        "    tfidf_matrix = vectorizer.fit_transform(np.array(text)).todense()\n",
        "    return tfidf_matrix\n",
        "\n",
        "tfidf_feat = tf_idf_extraction(clean_title)\n",
        "print(tfidf_feat[:3])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cLWq0bu5DVH"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-06T12:07:20.299121Z",
          "start_time": "2020-10-06T12:07:20.290299Z"
        },
        "id": "ZGthldne5DVI"
      },
      "source": [
        "category = df_data['Category'].astype('category').cat.codes # target variable"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtBronk85DVK"
      },
      "source": [
        "## No sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2020-10-06T12:21:38.160Z"
        },
        "id": "FG3wO81b5DVL",
        "outputId": "2034f1b9-5be6-471f-c997-9861bfbd2440",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "# list of features combinations\n",
        "feat_list = [np.hstack((ngram_feat, embed_feat)), np.hstack((tfidf_feat, embed_feat))]\n",
        "feat_name = ['tf and embedding', 'tf-idf and embedding']\n",
        "mnb = MultinomialNB()\n",
        "bnb = BernoulliNB()\n",
        "\n",
        "# list of model to do prediction\n",
        "model_list = [mnb, bnb]\n",
        "model_name = ['Multinomial Naive Bayes', 'Bernoulli Naive Bayes']\n",
        "\n",
        "# build the model and evaluate the performance of it for each feature combination\n",
        "df_recap = pd.DataFrame()\n",
        "for f, fn in zip(feat_list, feat_name):\n",
        "    print(\"Features : \", fn)\n",
        "    X = f\n",
        "    y = category\n",
        "    for m, n in zip(model_list, model_name):\n",
        "        scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro']\n",
        "        scores = cross_validate(m, X, y, cv=4, scoring=scoring)\n",
        "        acc = np.mean(scores['test_accuracy'])\n",
        "        f1 = np.mean(scores['test_f1_macro'])\n",
        "        precision = np.mean(scores['test_precision_macro'])\n",
        "        recall = np.mean(scores['test_recall_macro'])\n",
        "        print(\"Classifier : \", n)\n",
        "        print(\"Accuracy:\", acc)\n",
        "        print(\"F1-Measure:\", f1)\n",
        "        print(\"Precision:\", precision)\n",
        "        print(\"Recall:\", recall)\n",
        "        df_recap = df_recap.append({\n",
        "            'features': fn,\n",
        "            'classifier': n,\n",
        "            'accuracy': acc,\n",
        "            'f1_score': f1,\n",
        "            'precision': precision,\n",
        "            'recall': recall\n",
        "        }, ignore_index=True)\n",
        "        print('='*90)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features :  tf and embedding\n",
            "Classifier :  Multinomial Naive Bayes\n",
            "Accuracy: 0.8750743445681832\n",
            "F1-Measure: 0.862972451637912\n",
            "Precision: 0.8648583814099748\n",
            "Recall: 0.8612402290195584\n",
            "==========================================================================================\n",
            "Classifier :  Bernoulli Naive Bayes\n",
            "Accuracy: 0.8749827843827729\n",
            "F1-Measure: 0.8630592125250387\n",
            "Precision: 0.864534543396968\n",
            "Recall: 0.8616718862690685\n",
            "==========================================================================================\n",
            "Features :  tf-idf and embedding\n",
            "Classifier :  Multinomial Naive Bayes\n",
            "Accuracy: 0.8749827992848437\n",
            "F1-Measure: 0.8623230265997273\n",
            "Precision: 0.8780468486455774\n",
            "Recall: 0.85049955941954\n",
            "==========================================================================================\n",
            "Classifier :  Bernoulli Naive Bayes\n",
            "Accuracy: 0.8749827843827729\n",
            "F1-Measure: 0.8630592125250387\n",
            "Precision: 0.864534543396968\n",
            "Recall: 0.8616718862690685\n",
            "==========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pq3rBztWn_SN",
        "outputId": "fe22b98a-6407-4e36-be69-31af82d9057e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "# the recap of scenarios with no sampling\n",
        "df_recap"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>classifier</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>features</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.875074</td>\n",
              "      <td>Multinomial Naive Bayes</td>\n",
              "      <td>0.862972</td>\n",
              "      <td>tf and embedding</td>\n",
              "      <td>0.864858</td>\n",
              "      <td>0.861240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.874983</td>\n",
              "      <td>Bernoulli Naive Bayes</td>\n",
              "      <td>0.863059</td>\n",
              "      <td>tf and embedding</td>\n",
              "      <td>0.864535</td>\n",
              "      <td>0.861672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.874983</td>\n",
              "      <td>Multinomial Naive Bayes</td>\n",
              "      <td>0.862323</td>\n",
              "      <td>tf-idf and embedding</td>\n",
              "      <td>0.878047</td>\n",
              "      <td>0.850500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.874983</td>\n",
              "      <td>Bernoulli Naive Bayes</td>\n",
              "      <td>0.863059</td>\n",
              "      <td>tf-idf and embedding</td>\n",
              "      <td>0.864535</td>\n",
              "      <td>0.861672</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy               classifier  f1_score              features  \\\n",
              "0  0.875074  Multinomial Naive Bayes  0.862972      tf and embedding   \n",
              "1  0.874983    Bernoulli Naive Bayes  0.863059      tf and embedding   \n",
              "2  0.874983  Multinomial Naive Bayes  0.862323  tf-idf and embedding   \n",
              "3  0.874983    Bernoulli Naive Bayes  0.863059  tf-idf and embedding   \n",
              "\n",
              "   precision    recall  \n",
              "0   0.864858  0.861240  \n",
              "1   0.864535  0.861672  \n",
              "2   0.878047  0.850500  \n",
              "3   0.864535  0.861672  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r11LQvpD5DVN"
      },
      "source": [
        "## Undersampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-06T12:16:34.796338Z",
          "start_time": "2020-10-06T12:16:30.462395Z"
        },
        "id": "mHNxUi3h5DVO",
        "outputId": "6d6e5694-89c9-4a83-efb3-2a23ab30440b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        }
      },
      "source": [
        "# list of features combinations\n",
        "feat_list = [np.hstack((ngram_feat, embed_feat)), np.hstack((tfidf_feat, embed_feat))]\n",
        "feat_name = ['tf and embedding', 'tf-idf and embedding']\n",
        "mnb = MultinomialNB()\n",
        "bnb = BernoulliNB()\n",
        "\n",
        "# list of model to do prediction\n",
        "model_list = [mnb, bnb]\n",
        "model_name = ['Multinomial Naive Bayes', 'Bernoulli Naive Bayes']\n",
        "\n",
        "# build the model and evaluate the performance of it for each feature combination\n",
        "df_recap_undersample = pd.DataFrame()\n",
        "for f, fn in zip(feat_list, feat_name):\n",
        "    print(\"Features = \", fn)\n",
        "    X = f\n",
        "    y = category\n",
        "    under = RandomUnderSampler(random_state=0)\n",
        "    Xt, yt = under.fit_resample(X, y)\n",
        "    display(Counter(yt))\n",
        "    for m, n in zip(model_list, model_name):\n",
        "        scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro']\n",
        "        scores=cross_validate(m, Xt, yt, cv=4, scoring=scoring)\n",
        "        acc=np.mean(scores['test_accuracy'])\n",
        "        f1=np.mean(scores['test_f1_macro'])\n",
        "        precision=np.mean(scores['test_precision_macro'])\n",
        "        recall=np.mean(scores['test_recall_macro'])\n",
        "        print(\"Classifier : \", n)\n",
        "        print(\"Accuracy:\", acc)\n",
        "        print(\"F1-Measure:\", f1)\n",
        "        print(\"Precision:\", precision)\n",
        "        print(\"Recall:\", recall)\n",
        "        df_recap_undersample = df_recap_undersample.append({\n",
        "            'features': fn,\n",
        "            'classifier': n,\n",
        "            'accuracy': acc,\n",
        "            'f1_score': f1,\n",
        "            'precision': precision,\n",
        "            'recall': recall\n",
        "        }, ignore_index=True)\n",
        "        print('='*90)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features =  tf and embedding\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Counter({0: 7091, 1: 7091, 2: 7091, 3: 7091})"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Classifier :  Multinomial Naive Bayes\n",
            "Accuracy: 0.8628895783387391\n",
            "F1-Measure: 0.8626083980159777\n",
            "Precision: 0.8625976044133797\n",
            "Recall: 0.8628899133159926\n",
            "==========================================================================================\n",
            "Classifier :  Bernoulli Naive Bayes\n",
            "Accuracy: 0.8629248342969962\n",
            "F1-Measure: 0.8628363197817955\n",
            "Precision: 0.8628325521843135\n",
            "Recall: 0.8629253433430222\n",
            "==========================================================================================\n",
            "Features =  tf-idf and embedding\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Counter({0: 7091, 1: 7091, 2: 7091, 3: 7091})"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Classifier :  Multinomial Naive Bayes\n",
            "Accuracy: 0.863065858130024\n",
            "F1-Measure: 0.8628218543335067\n",
            "Precision: 0.8627525531783041\n",
            "Recall: 0.863066247824465\n",
            "==========================================================================================\n",
            "Classifier :  Bernoulli Naive Bayes\n",
            "Accuracy: 0.8629248342969962\n",
            "F1-Measure: 0.8628363197817955\n",
            "Precision: 0.8628325521843135\n",
            "Recall: 0.8629253433430222\n",
            "==========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcTUw-OtoHGS",
        "outputId": "b51f99f0-8eb6-41ff-e5e8-2f465f82594d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "# the recap of scenarios with undersampling\n",
        "df_recap_undersample"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>classifier</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>features</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.862890</td>\n",
              "      <td>Multinomial Naive Bayes</td>\n",
              "      <td>0.862608</td>\n",
              "      <td>tf and embedding</td>\n",
              "      <td>0.862598</td>\n",
              "      <td>0.862890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.862925</td>\n",
              "      <td>Bernoulli Naive Bayes</td>\n",
              "      <td>0.862836</td>\n",
              "      <td>tf and embedding</td>\n",
              "      <td>0.862833</td>\n",
              "      <td>0.862925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.863066</td>\n",
              "      <td>Multinomial Naive Bayes</td>\n",
              "      <td>0.862822</td>\n",
              "      <td>tf-idf and embedding</td>\n",
              "      <td>0.862753</td>\n",
              "      <td>0.863066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.862925</td>\n",
              "      <td>Bernoulli Naive Bayes</td>\n",
              "      <td>0.862836</td>\n",
              "      <td>tf-idf and embedding</td>\n",
              "      <td>0.862833</td>\n",
              "      <td>0.862925</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy               classifier  f1_score              features  \\\n",
              "0  0.862890  Multinomial Naive Bayes  0.862608      tf and embedding   \n",
              "1  0.862925    Bernoulli Naive Bayes  0.862836      tf and embedding   \n",
              "2  0.863066  Multinomial Naive Bayes  0.862822  tf-idf and embedding   \n",
              "3  0.862925    Bernoulli Naive Bayes  0.862836  tf-idf and embedding   \n",
              "\n",
              "   precision    recall  \n",
              "0   0.862598  0.862890  \n",
              "1   0.862833  0.862925  \n",
              "2   0.862753  0.863066  \n",
              "3   0.862833  0.862925  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODHtSwh-5DVQ"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    }
  ]
}