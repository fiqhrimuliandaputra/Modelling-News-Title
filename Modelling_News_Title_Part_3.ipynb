{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Modelling News Title - Part 3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "259.797px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikrSARUz1qbu"
      },
      "source": [
        "Alert! This script needs to access your GDrive\n",
        "\n",
        "Please upload the files to 'drive/My Drive/Colab Notebooks'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL5M-bBj1tsJ",
        "outputId": "57bb9496-7de9-4993-909c-74b67e0328cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW9HQ3cy2EO-",
        "outputId": "fc8aac95-9783-4632-9a98-3afce4f2ec59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install polyglot"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: polyglot in /usr/local/lib/python3.6/dist-packages (16.7.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF7uF3nS5DUd"
      },
      "source": [
        "# Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-06T12:03:58.538936Z",
          "start_time": "2020-10-06T12:03:58.045071Z"
        },
        "id": "tWgc-Asj5DUe",
        "outputId": "27237c91-fe92-4db6-a200-e18f26abec8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import numpy as np\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_validate\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "from polyglot.mapping import Embedding\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "pd.set_option('max_columns', 1000)\n",
        "pd.set_option('max_rows', 1000)\n",
        "\n",
        "punctuation = string.punctuation # list of punctuation\n",
        "digit = [i for i in range(0,10)] # list of digits \n",
        "\n",
        "english_stemmer = SnowballStemmer(\"english\", ignore_stopwords=True) # english stemmer\n",
        "en_stops = set(stopwords.words('english')) # english stopwords\n",
        "nlp = spacy.load(\"en_core_web_sm\") # model to do lemmatization\n",
        "words, embeddings = pickle.load(open('drive/My Drive/Colab Notebooks/polyglot-en.pkl', 'rb'), encoding='latin1') # word embedding from polyglot\n",
        "\n",
        "# Special tokens\n",
        "Token_ID = {\"<UNK>\": 0, \"<S>\": 1, \"</S>\":2, \"<PAD>\": 3}\n",
        "ID_Token = {v:k for k,v in Token_ID.items()}\n",
        "\n",
        "# Map words to indices and vice versa\n",
        "word_id = {w:i for (i, w) in enumerate(words)}\n",
        "id_word = dict(enumerate(words))\n",
        "\n",
        "# Normalize digits by replacing them with #\n",
        "DIGITS = re.compile(\"[0-9]\", re.UNICODE)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQFNe9AL5DUh"
      },
      "source": [
        "# Data import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-06T12:04:01.628003Z",
          "start_time": "2020-10-06T12:04:00.505159Z"
        },
        "id": "ceH1ctpH5DUi",
        "outputId": "512940d8-99fe-4f6a-884a-a0052f7426bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "df_data = pd.read_excel('./drive/My Drive/Colab Notebooks/News Title.xls')\n",
        "print(df_data.shape[0])\n",
        "df_data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65535\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>News Title</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Google+ rolls out 'Stories' for tricked out ph...</td>\n",
              "      <td>Technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Dov Charney's Redeeming Quality</td>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>White God adds Un Certain Regard to the Palm Dog</td>\n",
              "      <td>Entertainment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Google shows off Androids for wearables, cars,...</td>\n",
              "      <td>Technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>China May new bank loans at 870.8 bln yuan</td>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   No                                         News Title       Category\n",
              "0   1  Google+ rolls out 'Stories' for tricked out ph...     Technology\n",
              "1   2                    Dov Charney's Redeeming Quality       Business\n",
              "2   3   White God adds Un Certain Regard to the Palm Dog  Entertainment\n",
              "3   4  Google shows off Androids for wearables, cars,...     Technology\n",
              "4   5         China May new bank loans at 870.8 bln yuan       Business"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-06T12:04:04.471914Z",
          "start_time": "2020-10-06T12:04:04.447641Z"
        },
        "id": "1SmkfSoU5DUp",
        "outputId": "c1468d36-b6d8-4e5e-cf21-b07ce79fc84b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "df_data['Category'].value_counts(dropna=False)\n",
        "\n",
        "# the target class is imbalanced"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Entertainment    23961\n",
              "Business         17707\n",
              "Technology       16776\n",
              "Medical           7091\n",
              "Name: Category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kefqQdWS5DUs"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-06T12:04:06.248912Z",
          "start_time": "2020-10-06T12:04:06.228648Z"
        },
        "id": "_hssRBVU5DUs"
      },
      "source": [
        "'''\n",
        "Function to clean the data, which includes:\n",
        "1. Lowercasing\n",
        "2. Punctuation removal\n",
        "3. Digit removal\n",
        "'''  \n",
        "def cleaning(text):\n",
        "    # lowercase\n",
        "    normal = text.lower()\n",
        "    # remove punctuation\n",
        "    normal = re.sub(r'[^\\w\\s]', '', normal) \n",
        "    # remove numbers\n",
        "    normal = re.sub(r'\\d+', ' ', normal)\n",
        "    return normal\n",
        "\n",
        "'''\n",
        "Function to normalize the form of the token (lemmatization)\n",
        "and to remove stopwords\n",
        "'''\n",
        "def normalize_and_remove_stopwords(text):\n",
        "    tokens = nlp(text)\n",
        "    token_new = []\n",
        "    \n",
        "    for k in tokens:\n",
        "        if k.lemma_ not in en_stops:\n",
        "            token_new.append(k.lemma_)\n",
        "\n",
        "    str_clean = ' '.join(token_new)\n",
        "    return str_clean\n",
        "\n",
        "'''\n",
        "Function to do stemming, in this case, we use lemmatization\n",
        "instead of stemming\n",
        "'''\n",
        "def stemming(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    stem_sentence = []\n",
        "    for k in tokens:\n",
        "        stem_word = english_stemmer.stem(k)\n",
        "        stem_sentence.append(stem_word)\n",
        "\n",
        "    stem_sentence_str = ' '.join(stem_sentence)\n",
        "    return stem_sentence_str\n",
        "\n",
        "'''\n",
        "Data preprocessing function, which includes:\n",
        "1. Text cleaning,\n",
        "2. Text normalization, and\n",
        "3. Stopword removal\n",
        "'''\n",
        "def preprocessing(list_text):\n",
        "    text_clean = []\n",
        "    for t in list_text:\n",
        "        normal = cleaning(t)\n",
        "#         normal = stemming(normal)\n",
        "        normal = normalize_and_remove_stopwords(normal)\n",
        "        text_clean.append(normal)\n",
        "    return text_clean"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-06T05:27:23.917328Z",
          "start_time": "2020-10-06T05:18:24.771949Z"
        },
        "id": "DDC7udnv5DUv",
        "outputId": "74f88e1d-826b-478d-b364-ee10b6daf112",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "raw_text = df_data['News Title']\n",
        "\n",
        "clean_text = preprocessing(raw_text) # do the preprocessing\n",
        "clean_text[:3]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['google roll story trick photo playback',\n",
              " 'dov charney redeem quality',\n",
              " 'white god add un certain regard palm dog']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-06T05:28:28.175841Z",
          "start_time": "2020-10-06T05:28:28.077284Z"
        },
        "id": "sj3OwUoh5DUz"
      },
      "source": [
        "# save the clean comments to csv, so we can use it later on\n",
        "df_clean_title = pd.DataFrame(clean_text, columns=['title'])\n",
        "df_clean_title.to_csv('drive/My Drive/Colab Notebooks/df_clean_title_no_stemming.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXVZZPq18q-8"
      },
      "source": [
        "df_clean_title = pd.read_csv('drive/My Drive/Colab Notebooks/df_clean_title_no_stemming.csv')\n",
        "clean_title = df_clean_title['title'] # clean text"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7Z95YKJ5DU7"
      },
      "source": [
        "# Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-06T12:04:13.593122Z",
          "start_time": "2020-10-06T12:04:12.930113Z"
        },
        "id": "9B2THLuf5DU7",
        "outputId": "e0daa417-906d-4a43-c0dd-57a2ffb1afbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "'''\n",
        "Function to extract TF (1-gram) features\n",
        "'''\n",
        "def tf_extraction(text, ngram_start, ngram_end):\n",
        "    ngram = CountVectorizer(ngram_range=(ngram_start, ngram_end), max_features=3000)\n",
        "    ngram_matrix = ngram.fit_transform(np.array(text)).todense()\n",
        "    return ngram_matrix\n",
        "\n",
        "# unigram features\n",
        "ngram_feat = tf_extraction(clean_title, 1, 1)\n",
        "print(ngram_feat[:3])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Sqqo4IU87fu"
      },
      "source": [
        "'''\n",
        "In case the word is not available in the vocabulary,\n",
        "we can try multiple case normalizing procedure.\n",
        "We consider the best substitute to be the one with the lowest index,\n",
        "which is equivalent to the most frequent alternative.\n",
        "\n",
        "Source: https://nbviewer.jupyter.org/gist/aboSamoor/6046170\n",
        "''' \n",
        "def case_normalizer(word, dictionary):\n",
        "    w = word\n",
        "    lower = (dictionary.get(w.lower(), 1e2), w.lower())\n",
        "    upper = (dictionary.get(w.upper(), 1e2), w.upper())\n",
        "    title = (dictionary.get(w.title(), 1e2), w.title())\n",
        "    results = [lower, upper, title]\n",
        "    results.sort()\n",
        "    index, w = results[0]\n",
        "    if index != 1e2:\n",
        "        return w\n",
        "    return word\n",
        "\n",
        "'''\n",
        "Find the closest alternative in case the word is OOV.\n",
        "\n",
        "Source: https://nbviewer.jupyter.org/gist/aboSamoor/6046170\n",
        "'''\n",
        "def normalize(word, word_id):\n",
        "    if not word in word_id:\n",
        "        word = DIGITS.sub(\"#\", word)\n",
        "    if not word in word_id:\n",
        "        word = case_normalizer(word, word_id)\n",
        "\n",
        "    if not word in word_id:\n",
        "        return None\n",
        "    return word"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-06T12:04:25.385975Z",
          "start_time": "2020-10-06T12:04:25.370092Z"
        },
        "id": "2YKqFu6c5DU-"
      },
      "source": [
        "'''\n",
        "Function to retrieve the Euclidean distance\n",
        "between two words in the embedding vectors \n",
        "'''\n",
        "def get_distance(word1, word2, embeddings, word_id, id_word):\n",
        "    word1 = normalize(word1, word_id)\n",
        "    word2 = normalize(word2, word_id)\n",
        "    if not word1 or not word2:\n",
        "        return 1e2\n",
        "    word1_index = word_id[word1]\n",
        "    word2_index = word_id[word2]\n",
        "    e1 = embeddings[word1_index]\n",
        "    e2 = embeddings[word2_index]\n",
        "    distance = ((e2 - e1) ** 2).sum() ** 0.5\n",
        "    return distance"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-06T12:06:07.610604Z",
          "start_time": "2020-10-06T12:05:44.202973Z"
        },
        "id": "BvobSK8H5DVB",
        "outputId": "74e9146d-ab78-4ab4-9ba2-1213fbfce4d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "'''\n",
        "Function to extract the embedding features, which include minimum distance of text to:\n",
        "1. Entertainment word\n",
        "2. Technology word\n",
        "3. Medical word\n",
        "4. Business word\n",
        "'''\n",
        "def embedding_extraction(text):\n",
        "    all_embedding_feat = []\n",
        "    for t in text:\n",
        "        entertainment_distances = []\n",
        "        technology_distances = []\n",
        "        medical_distances = []\n",
        "        business_distances = []\n",
        "        token = nltk.word_tokenize(t)\n",
        "        for k in token:\n",
        "            entertainment_distances.append(get_distance(k, 'entertainment', embeddings, word_id, id_word))\n",
        "            technology_distances.append(get_distance(k, 'technology', embeddings, word_id, id_word))\n",
        "            medical_distances.append(get_distance(k, 'medical', embeddings, word_id, id_word))\n",
        "            business_distances.append(get_distance(k, 'business', embeddings, word_id, id_word))\n",
        "        all_embedding_feat.append([min(entertainment_distances), min(technology_distances), min(medical_distances), min(business_distances)])\n",
        "    return all_embedding_feat\n",
        "\n",
        "embed_feat = embedding_extraction(clean_title)\n",
        "embed_feat[:3]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2.913120148087336, 2.948760506100634, 3.5125143573508284, 3.091267832678769],\n",
              " [2.4571364339319977,\n",
              "  3.0249680383428195,\n",
              "  3.3134951266334514,\n",
              "  2.932971141591275],\n",
              " [3.041295033226138,\n",
              "  3.5203397205450035,\n",
              "  3.7982305924660396,\n",
              "  3.269813735043243]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lf3sjM4sLcl",
        "outputId": "d30c15a0-65c3-496e-c8d7-a50658d1f121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'''\n",
        "Function to extract orthography and url occurence features\n",
        "'''\n",
        "def ortography(text):\n",
        "    all_orto_feat = []\n",
        "    for t in text:\n",
        "        capital_count = sum(1 for c in t if c.isupper())\n",
        "        exclamation_count = sum(1 for c in t if c == \"!\")\n",
        "        punctuation_count = sum(1 for c in t if c in punctuation)\n",
        "        word_len = len(nltk.word_tokenize(t))\n",
        "        char_len = len(t)\n",
        "        digit_occurence = sum(1 for c in t if c in digit)\n",
        "        orto_feat = [capital_count, exclamation_count, punctuation_count, word_len, char_len, digit_occurence]\n",
        "        all_orto_feat.append(orto_feat)\n",
        "    return all_orto_feat\n",
        "\n",
        "orto_feat = ortography(df_data['News Title'])\n",
        "orto_feat[:3]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 0, 3, 10, 58, 0], [4, 0, 1, 5, 31, 0], [7, 0, 0, 10, 48, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-06T12:07:17.530517Z",
          "start_time": "2020-10-06T12:07:16.746239Z"
        },
        "id": "cdCIowsf5DVF",
        "outputId": "e2dfb971-404b-4d28-8b19-35a6d1486aa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "'''\n",
        "Function to extract TF-IDF (1-gram) features\n",
        "'''\n",
        "def tf_idf_extraction(text):\n",
        "    vectorizer = TfidfVectorizer(max_features=3000)\n",
        "    tfidf_matrix = vectorizer.fit_transform(np.array(text)).todense()\n",
        "    return tfidf_matrix\n",
        "\n",
        "# tf-idf features\n",
        "tfidf_feat = tf_idf_extraction(clean_title)\n",
        "print(tfidf_feat[:3])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cLWq0bu5DVH"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-06T12:07:20.299121Z",
          "start_time": "2020-10-06T12:07:20.290299Z"
        },
        "id": "ZGthldne5DVI"
      },
      "source": [
        "category = df_data['Category'].astype('category').cat.codes # target variable"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtBronk85DVK"
      },
      "source": [
        "## No sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2020-10-06T12:21:38.160Z"
        },
        "id": "FG3wO81b5DVL",
        "outputId": "480739f6-896b-403f-d592-6d12e4ab9fbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "# list of features combinations\n",
        "feat_list = [np.hstack((ngram_feat, orto_feat, embed_feat)), np.hstack((tfidf_feat, orto_feat, embed_feat))]\n",
        "feat_name = ['tf, orthography, and embedding', 'tf-idf, orthography, and embedding']\n",
        "mnb = MultinomialNB()\n",
        "bnb = BernoulliNB()\n",
        "\n",
        "# list of model to do prediction\n",
        "model_list = [mnb, bnb]\n",
        "model_name = ['Multinomial Naive Bayes', 'Bernoulli Naive Bayes']\n",
        "\n",
        "# build the model and evaluate the performance of it for each feature combination\n",
        "df_recap = pd.DataFrame()\n",
        "for f, fn in zip(feat_list, feat_name):\n",
        "    print(\"Jenis Fitur = \", fn)\n",
        "    X = f\n",
        "    y = category\n",
        "    for m, n in zip(model_list, model_name):\n",
        "        scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro']\n",
        "        scores = cross_validate(m, X, y, cv=4, scoring=scoring)\n",
        "        acc = np.mean(scores['test_accuracy'])\n",
        "        f1 = np.mean(scores['test_f1_macro'])\n",
        "        precision = np.mean(scores['test_precision_macro'])\n",
        "        recall = np.mean(scores['test_recall_macro'])\n",
        "        print(\"Jenis Classifier : \", n)\n",
        "        print(\"Accuracy:\", acc)\n",
        "        print(\"F1-Measure:\", f1)\n",
        "        print(\"Precision:\", precision)\n",
        "        print(\"Recall:\", recall)\n",
        "        df_recap = df_recap.append({\n",
        "            'features': fn,\n",
        "            'classifier': n,\n",
        "            'accuracy': acc,\n",
        "            'f1_score': f1,\n",
        "            'precision': precision,\n",
        "            'recall': recall\n",
        "        }, ignore_index=True)\n",
        "        print('='*90)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Jenis Fitur =  tf, orthography, and embedding\n",
            "Jenis Classifier :  Multinomial Naive Bayes\n",
            "Accuracy: 0.8737010591408348\n",
            "F1-Measure: 0.8613290113035857\n",
            "Precision: 0.8607781396074305\n",
            "Recall: 0.8619685843820468\n",
            "==========================================================================================\n",
            "Jenis Classifier :  Bernoulli Naive Bayes\n",
            "Accuracy: 0.8750438195390229\n",
            "F1-Measure: 0.8633121105946644\n",
            "Precision: 0.8640802924130746\n",
            "Recall: 0.8625997256083644\n",
            "==========================================================================================\n",
            "Jenis Fitur =  tf-idf, orthography, and embedding\n",
            "Jenis Classifier :  Multinomial Naive Bayes\n",
            "Accuracy: 0.8653238796310271\n",
            "F1-Measure: 0.851028706326597\n",
            "Precision: 0.8601854579921212\n",
            "Recall: 0.8434438993939989\n",
            "==========================================================================================\n",
            "Jenis Classifier :  Bernoulli Naive Bayes\n",
            "Accuracy: 0.8750438195390229\n",
            "F1-Measure: 0.8633121105946644\n",
            "Precision: 0.8640802924130746\n",
            "Recall: 0.8625997256083644\n",
            "==========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emEs5NBynjTa",
        "outputId": "5503218a-1e52-462a-f188-f8e5a340d9f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "df_recap"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>classifier</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>features</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.873701</td>\n",
              "      <td>Multinomial Naive Bayes</td>\n",
              "      <td>0.861329</td>\n",
              "      <td>tf, orthography, and embedding</td>\n",
              "      <td>0.860778</td>\n",
              "      <td>0.861969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.875044</td>\n",
              "      <td>Bernoulli Naive Bayes</td>\n",
              "      <td>0.863312</td>\n",
              "      <td>tf, orthography, and embedding</td>\n",
              "      <td>0.864080</td>\n",
              "      <td>0.862600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.865324</td>\n",
              "      <td>Multinomial Naive Bayes</td>\n",
              "      <td>0.851029</td>\n",
              "      <td>tf-idf, orthography, and embedding</td>\n",
              "      <td>0.860185</td>\n",
              "      <td>0.843444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.875044</td>\n",
              "      <td>Bernoulli Naive Bayes</td>\n",
              "      <td>0.863312</td>\n",
              "      <td>tf-idf, orthography, and embedding</td>\n",
              "      <td>0.864080</td>\n",
              "      <td>0.862600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy               classifier  f1_score  \\\n",
              "0  0.873701  Multinomial Naive Bayes  0.861329   \n",
              "1  0.875044    Bernoulli Naive Bayes  0.863312   \n",
              "2  0.865324  Multinomial Naive Bayes  0.851029   \n",
              "3  0.875044    Bernoulli Naive Bayes  0.863312   \n",
              "\n",
              "                             features  precision    recall  \n",
              "0      tf, orthography, and embedding   0.860778  0.861969  \n",
              "1      tf, orthography, and embedding   0.864080  0.862600  \n",
              "2  tf-idf, orthography, and embedding   0.860185  0.843444  \n",
              "3  tf-idf, orthography, and embedding   0.864080  0.862600  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r11LQvpD5DVN"
      },
      "source": [
        "## Undersampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-06T12:16:34.796338Z",
          "start_time": "2020-10-06T12:16:30.462395Z"
        },
        "id": "mHNxUi3h5DVO",
        "outputId": "32e381a2-ca87-4f80-86a8-7daa09723882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        }
      },
      "source": [
        "# list of features combinations\n",
        "feat_list = [np.hstack((ngram_feat, orto_feat, embed_feat)), np.hstack((tfidf_feat, orto_feat, embed_feat))]\n",
        "feat_name = ['tf, orthography, and embedding', 'tf-idf, orthography, and embedding']\n",
        "mnb = MultinomialNB()\n",
        "bnb = BernoulliNB()\n",
        "\n",
        "# list of model to do prediction\n",
        "model_list = [mnb, bnb]\n",
        "model_name = ['Multinomial Naive Bayes', 'Bernoulli Naive Bayes']\n",
        "\n",
        "# build the model and evaluate the performance of it for each feature combination\n",
        "df_recap_undersample = pd.DataFrame()\n",
        "for f, fn in zip(feat_list, feat_name):\n",
        "    print(\"Features : \", fn)\n",
        "    X = f\n",
        "    y = category\n",
        "    under = RandomUnderSampler(random_state=0)\n",
        "    Xt, yt = under.fit_resample(X, y)\n",
        "    display(Counter(yt))\n",
        "    for m, n in zip(model_list, model_name):\n",
        "        scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro']\n",
        "        scores=cross_validate(m, Xt, yt, cv=4, scoring=scoring)\n",
        "        acc=np.mean(scores['test_accuracy'])\n",
        "        f1=np.mean(scores['test_f1_macro'])\n",
        "        precision=np.mean(scores['test_precision_macro'])\n",
        "        recall=np.mean(scores['test_recall_macro'])\n",
        "        print(\"Classifier : \", n)\n",
        "        print(\"Accuracy:\", acc)\n",
        "        print(\"F1-Measure:\", f1)\n",
        "        print(\"Precision:\", precision)\n",
        "        print(\"Recall:\", recall)\n",
        "        df_recap_undersample = df_recap_undersample.append({\n",
        "            'features': fn,\n",
        "            'classifier': n,\n",
        "            'accuracy': acc,\n",
        "            'f1_score': f1,\n",
        "            'precision': precision,\n",
        "            'recall': recall\n",
        "        }, ignore_index=True)\n",
        "        print('='*90)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features :  tf, orthography, and embedding\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Counter({0: 7091, 1: 7091, 2: 7091, 3: 7091})"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Classifier :  Multinomial Naive Bayes\n",
            "Accuracy: 0.8586588633479059\n",
            "F1-Measure: 0.8583872147930922\n",
            "Precision: 0.85835960760747\n",
            "Recall: 0.8586593373260049\n",
            "==========================================================================================\n",
            "Classifier :  Bernoulli Naive Bayes\n",
            "Accuracy: 0.8619729234240587\n",
            "F1-Measure: 0.8618853588565973\n",
            "Precision: 0.8619037061694187\n",
            "Recall: 0.8619735269066089\n",
            "==========================================================================================\n",
            "Features :  tf-idf, orthography, and embedding\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Counter({0: 7091, 1: 7091, 2: 7091, 3: 7091})"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Classifier :  Multinomial Naive Bayes\n",
            "Accuracy: 0.8410661401776901\n",
            "F1-Measure: 0.8408113777516818\n",
            "Precision: 0.8409503486074156\n",
            "Recall: 0.8410667672791903\n",
            "==========================================================================================\n",
            "Classifier :  Bernoulli Naive Bayes\n",
            "Accuracy: 0.8619729234240587\n",
            "F1-Measure: 0.8618853588565973\n",
            "Precision: 0.8619037061694187\n",
            "Recall: 0.8619735269066089\n",
            "==========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODHtSwh-5DVQ",
        "outputId": "00d72c18-a11a-4829-b2e7-98981d6c4e5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "# the recap of scenarios with undersampling\n",
        "df_recap_undersample"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>classifier</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>features</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.858659</td>\n",
              "      <td>Multinomial Naive Bayes</td>\n",
              "      <td>0.858387</td>\n",
              "      <td>tf, orthography, and embedding</td>\n",
              "      <td>0.858360</td>\n",
              "      <td>0.858659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.861973</td>\n",
              "      <td>Bernoulli Naive Bayes</td>\n",
              "      <td>0.861885</td>\n",
              "      <td>tf, orthography, and embedding</td>\n",
              "      <td>0.861904</td>\n",
              "      <td>0.861974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.841066</td>\n",
              "      <td>Multinomial Naive Bayes</td>\n",
              "      <td>0.840811</td>\n",
              "      <td>tf-idf, orthography, and embedding</td>\n",
              "      <td>0.840950</td>\n",
              "      <td>0.841067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.861973</td>\n",
              "      <td>Bernoulli Naive Bayes</td>\n",
              "      <td>0.861885</td>\n",
              "      <td>tf-idf, orthography, and embedding</td>\n",
              "      <td>0.861904</td>\n",
              "      <td>0.861974</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy               classifier  f1_score  \\\n",
              "0  0.858659  Multinomial Naive Bayes  0.858387   \n",
              "1  0.861973    Bernoulli Naive Bayes  0.861885   \n",
              "2  0.841066  Multinomial Naive Bayes  0.840811   \n",
              "3  0.861973    Bernoulli Naive Bayes  0.861885   \n",
              "\n",
              "                             features  precision    recall  \n",
              "0      tf, orthography, and embedding   0.858360  0.858659  \n",
              "1      tf, orthography, and embedding   0.861904  0.861974  \n",
              "2  tf-idf, orthography, and embedding   0.840950  0.841067  \n",
              "3  tf-idf, orthography, and embedding   0.861904  0.861974  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gEjoelJnnXh"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    }
  ]
}