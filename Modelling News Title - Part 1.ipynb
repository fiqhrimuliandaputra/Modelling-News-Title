{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Modelling News Title - Part 1.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"259.797px"},"toc_section_display":true,"toc_window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"id":"l2wV2hSQmWmu"},"source":["Alert! This script needs to access your GDrive\n","\n","Please upload the files to 'drive/My Drive/Colab Notebooks'"]},{"cell_type":"code","metadata":{"id":"9rw2sLQ2mHCe","executionInfo":{"status":"ok","timestamp":1602150264608,"user_tz":-480,"elapsed":30965,"user":{"displayName":"Hadi Syah Putra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFq8cRfVCtoWipwZWKSOI_noyvowyse5zRadkS2g=s64","userId":"02288041668049318883"}},"outputId":"5184d682-6cf0-4abb-b18b-925db6c512d0","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aF7uF3nS5DUd"},"source":["# Library"]},{"cell_type":"code","metadata":{"id":"d7_TW8Ohh5x6","executionInfo":{"status":"ok","timestamp":1602149153995,"user_tz":-480,"elapsed":6943,"user":{"displayName":"Hadi Syah Putra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFq8cRfVCtoWipwZWKSOI_noyvowyse5zRadkS2g=s64","userId":"02288041668049318883"}},"outputId":"f80ee855-edb3-4e5b-abb9-5f1b2848e6b8","colab":{"base_uri":"https://localhost:8080/","height":207}},"source":["!pip install polyglot"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting polyglot\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/98/e24e2489114c5112b083714277204d92d372f5bbe00d5507acf40370edb9/polyglot-16.7.4.tar.gz (126kB)\n","\r\u001b[K     |██▋                             | 10kB 16.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20kB 6.2MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 81kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 112kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 8.6MB/s \n","\u001b[?25hBuilding wheels for collected packages: polyglot\n","  Building wheel for polyglot (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for polyglot: filename=polyglot-16.7.4-py2.py3-none-any.whl size=52559 sha256=3da591f506c6a441016fff210db7af49c421c8783c0375058e29bba5f01d2438\n","  Stored in directory: /root/.cache/pip/wheels/5e/91/ef/f1369fdc1203b0a9347d4b24f149b83a305f39ab047986d9da\n","Successfully built polyglot\n","Installing collected packages: polyglot\n","Successfully installed polyglot-16.7.4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-10-06T12:03:58.538936Z","start_time":"2020-10-06T12:03:58.045071Z"},"id":"tWgc-Asj5DUe","executionInfo":{"status":"ok","timestamp":1602150409728,"user_tz":-480,"elapsed":2963,"user":{"displayName":"Hadi Syah Putra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFq8cRfVCtoWipwZWKSOI_noyvowyse5zRadkS2g=s64","userId":"02288041668049318883"}},"outputId":"4bd7bedd-7bf1-42b1-cfb9-4889265dc770","colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["import pandas as pd\n","import re\n","from collections import Counter\n","import pickle\n","import numpy as np\n","import string\n","\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","from nltk.stem.snowball import SnowballStemmer\n","import spacy\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.pipeline import Pipeline\n","\n","from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import cross_validate\n","from imblearn.under_sampling import RandomUnderSampler\n","\n","from polyglot.mapping import Embedding\n","from gensim.models import Word2Vec\n","\n","pd.set_option('max_columns', 1000)\n","pd.set_option('max_rows', 1000)\n","\n","punctuation = string.punctuation # list of punctuation\n","digit = [i for i in range(0,10)] # list of digits \n","\n","english_stemmer = SnowballStemmer(\"english\", ignore_stopwords=True) # english stemmer\n","en_stops = set(stopwords.words('english')) # english stopwords\n","nlp = spacy.load(\"en_core_web_sm\") # model to do lemmatization\n","\n","words, embeddings = pickle.load(open('drive/My Drive/Colab Notebooks/polyglot-en.pkl', 'rb'), encoding='latin1') # word embedding from polyglot\n","\n","# Special tokens\n","Token_ID = {\"<UNK>\": 0, \"<S>\": 1, \"</S>\":2, \"<PAD>\": 3}\n","ID_Token = {v:k for k,v in Token_ID.items()}\n","\n","# Map words to indices and vice versa\n","word_id = {w:i for (i, w) in enumerate(words)}\n","id_word = dict(enumerate(words))\n","\n","# Normalize digits by replacing them with #\n","DIGITS = re.compile(\"[0-9]\", re.UNICODE)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aQFNe9AL5DUh"},"source":["# Data import"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-10-06T12:04:01.628003Z","start_time":"2020-10-06T12:04:00.505159Z"},"id":"ceH1ctpH5DUi","executionInfo":{"status":"ok","timestamp":1602150423124,"user_tz":-480,"elapsed":5274,"user":{"displayName":"Hadi Syah Putra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFq8cRfVCtoWipwZWKSOI_noyvowyse5zRadkS2g=s64","userId":"02288041668049318883"}},"outputId":"7fd8fa8d-ee66-4f20-c999-4695b056e0e9","colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["df_data = pd.read_excel('drive/My Drive/Colab Notebooks/News Title.xls')\n","print(df_data.shape[0])\n","df_data.head()"],"execution_count":12,"outputs":[{"output_type":"stream","text":["65535\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>No</th>\n","      <th>News Title</th>\n","      <th>Category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Google+ rolls out 'Stories' for tricked out ph...</td>\n","      <td>Technology</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Dov Charney's Redeeming Quality</td>\n","      <td>Business</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>White God adds Un Certain Regard to the Palm Dog</td>\n","      <td>Entertainment</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>Google shows off Androids for wearables, cars,...</td>\n","      <td>Technology</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>China May new bank loans at 870.8 bln yuan</td>\n","      <td>Business</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   No                                         News Title       Category\n","0   1  Google+ rolls out 'Stories' for tricked out ph...     Technology\n","1   2                    Dov Charney's Redeeming Quality       Business\n","2   3   White God adds Un Certain Regard to the Palm Dog  Entertainment\n","3   4  Google shows off Androids for wearables, cars,...     Technology\n","4   5         China May new bank loans at 870.8 bln yuan       Business"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-10-06T12:04:04.471914Z","start_time":"2020-10-06T12:04:04.447641Z"},"id":"1SmkfSoU5DUp","executionInfo":{"status":"ok","timestamp":1602150425365,"user_tz":-480,"elapsed":1141,"user":{"displayName":"Hadi Syah Putra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFq8cRfVCtoWipwZWKSOI_noyvowyse5zRadkS2g=s64","userId":"02288041668049318883"}},"outputId":"96b9a242-4571-4235-ebc6-9b3d829265e3","colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["df_data['Category'].value_counts(dropna=False)\n","\n","# the target class is imbalanced"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Entertainment    23961\n","Business         17707\n","Technology       16776\n","Medical           7091\n","Name: Category, dtype: int64"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"kefqQdWS5DUs"},"source":["# Preprocessing"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-10-06T12:04:06.248912Z","start_time":"2020-10-06T12:04:06.228648Z"},"id":"_hssRBVU5DUs"},"source":["'''\n","Function to clean the data, which includes:\n","1. Lowercasing\n","2. Punctuation removal\n","3. Digit removal\n","''' \n","def cleaning(text):\n","    # lowercase\n","    normal = text.lower()\n","    # remove punctuation\n","    normal = re.sub(r'[^\\w\\s]', '', normal) \n","    # remove numbers\n","    normal = re.sub(r'\\d+', ' ', normal)\n","    return normal\n","\n","'''\n","Function to normalize the form of the token (lemmatization)\n","and to remove stopwords\n","'''\n","def normalize_and_remove_stopwords(text):\n","    tokens = nlp(text)\n","    token_new = []\n","    \n","    for k in tokens:\n","        if k.lemma_ not in en_stops:\n","            token_new.append(k.lemma_)\n","\n","    str_clean = ' '.join(token_new)\n","    return str_clean\n","\n","'''\n","Function to do stemming, in this case, we use lemmatization\n","instead of stemming\n","'''\n","def stemming(text):\n","    tokens = nltk.word_tokenize(text)\n","    stem_sentence = []\n","    for k in tokens:\n","        stem_word = english_stemmer.stem(k)\n","        stem_sentence.append(stem_word)\n","\n","    stem_sentence_str = ' '.join(stem_sentence)\n","    return stem_sentence_str\n","\n","'''\n","Data preprocessing function, which includes:\n","1. Text cleaning,\n","2. Text normalization, and\n","3. Stopword removal\n","'''\n","def preprocessing(list_text):\n","    text_clean = []\n","    for t in list_text:\n","        normal = cleaning(t)\n","#         normal = stemming(normal)\n","        normal = normalize_and_remove_stopwords(normal)\n","        text_clean.append(normal)\n","    return text_clean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-10-06T05:27:23.917328Z","start_time":"2020-10-06T05:18:24.771949Z"},"id":"DDC7udnv5DUv","outputId":"27401dfd-f21a-48d8-c517-ff41bde15282","colab":{"base_uri":"https://localhost:8080/"}},"source":["raw_text = df_data['News Title']\n","\n","clean_text = preprocessing(raw_text) # do the preprocessing\n","clean_text[:3]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['google roll story trick photo playback',\n"," 'dov charney redeem quality',\n"," 'white god add un certain regard palm dog']"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-10-06T05:28:28.175841Z","start_time":"2020-10-06T05:28:28.077284Z"},"id":"sj3OwUoh5DUz"},"source":["# save the clean comments to csv, so we can use it later on\n","df_clean_title = pd.DataFrame(clean_text, columns=['title'])\n","df_clean_title.to_csv('drive/My Drive/Colab Notebooks/df_clean_title_no_stemming.csv', index=False, encoding='utf-8')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aXVZZPq18q-8","executionInfo":{"status":"ok","timestamp":1602150492747,"user_tz":-480,"elapsed":1933,"user":{"displayName":"Hadi Syah Putra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFq8cRfVCtoWipwZWKSOI_noyvowyse5zRadkS2g=s64","userId":"02288041668049318883"}}},"source":["df_clean_title = pd.read_csv('drive/My Drive/Colab Notebooks/df_clean_title_no_stemming.csv')\n","clean_title = df_clean_title['title'] # clean text"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A7Z95YKJ5DU7"},"source":["# Feature extraction"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-10-06T12:04:13.593122Z","start_time":"2020-10-06T12:04:12.930113Z"},"id":"9B2THLuf5DU7","executionInfo":{"status":"ok","timestamp":1602150500340,"user_tz":-480,"elapsed":3112,"user":{"displayName":"Hadi Syah Putra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFq8cRfVCtoWipwZWKSOI_noyvowyse5zRadkS2g=s64","userId":"02288041668049318883"}},"outputId":"9d63c38e-3e18-4305-88f7-4ba3fcb1b744","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["'''\n","Function to extract TF (1-gram) features\n","'''\n","def tf_extraction(text, ngram_start, ngram_end):\n","    ngram = CountVectorizer(ngram_range=(ngram_start, ngram_end), max_features=3000)\n","    ngram_matrix = ngram.fit_transform(np.array(text)).todense()\n","    return ngram_matrix\n","\n","# unigram features\n","ngram_feat = tf_extraction(clean_title, 1, 1)\n","print(ngram_feat[:3])"],"execution_count":17,"outputs":[{"output_type":"stream","text":["[[0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4Sqqo4IU87fu","executionInfo":{"status":"ok","timestamp":1602150500343,"user_tz":-480,"elapsed":1619,"user":{"displayName":"Hadi Syah Putra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFq8cRfVCtoWipwZWKSOI_noyvowyse5zRadkS2g=s64","userId":"02288041668049318883"}}},"source":["'''\n","In case the word is not available in the vocabulary,\n","we can try multiple case normalizing procedure.\n","We consider the best substitute to be the one with the lowest index,\n","which is equivalent to the most frequent alternative.\n","\n","Source: https://nbviewer.jupyter.org/gist/aboSamoor/6046170\n","''' \n","def case_normalizer(word, dictionary):\n","    w = word\n","    lower = (dictionary.get(w.lower(), 1e2), w.lower())\n","    upper = (dictionary.get(w.upper(), 1e2), w.upper())\n","    title = (dictionary.get(w.title(), 1e2), w.title())\n","    results = [lower, upper, title]\n","    results.sort()\n","    index, w = results[0]\n","    if index != 1e2:\n","        return w\n","    return word\n","\n","'''\n","Find the closest alternative in case the word is OOV.\n","\n","Source: https://nbviewer.jupyter.org/gist/aboSamoor/6046170\n","'''\n","def normalize(word, word_id):''' \n","'''\n","    if not word in word_id:\n","        word = DIGITS.sub(\"#\", word)\n","    if not word in word_id:\n","        word = case_normalizer(word, word_id)\n","\n","    if not word in word_id:\n","        return None\n","    return word"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-10-06T12:04:25.385975Z","start_time":"2020-10-06T12:04:25.370092Z"},"id":"2YKqFu6c5DU-","executionInfo":{"status":"ok","timestamp":1602150501653,"user_tz":-480,"elapsed":1155,"user":{"displayName":"Hadi Syah Putra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFq8cRfVCtoWipwZWKSOI_noyvowyse5zRadkS2g=s64","userId":"02288041668049318883"}}},"source":["'''\n","Function to retrieve the Euclidean distance\n","between two words in the embedding vectors \n","'''\n","def get_distance(word1, word2, embeddings, word_id, id_word):\n","    word1 = normalize(word1, word_id)\n","    word2 = normalize(word2, word_id)\n","    if not word1 or not word2: # if word 1 or word 2 not found in the embedding vectors, return 100 as the distance score\n","        return 1e2\n","    word1_index = word_id[word1]\n","    word2_index = word_id[word2]\n","    e1 = embeddings[word1_index]\n","    e2 = embeddings[word2_index]\n","    distance = ((e2 - e1) ** 2).sum() ** 0.5\n","    return distance"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-10-08T02:07:43.412113Z","start_time":"2020-10-08T02:07:43.199033Z"},"id":"BvobSK8H5DVB","executionInfo":{"status":"ok","timestamp":1602150550924,"user_tz":-480,"elapsed":23959,"user":{"displayName":"Hadi Syah Putra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFq8cRfVCtoWipwZWKSOI_noyvowyse5zRadkS2g=s64","userId":"02288041668049318883"}},"outputId":"f97b5dd8-f886-40ef-cb4f-fc632cac62d0","colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["'''\n","Function to extract the embedding features, which include minimum distance of text to:\n","1. Entertainment word\n","2. Technology word\n","3. Medical word\n","4. Business word\n","'''\n","def embedding_extraction(text):\n","    all_embedding_feat = []\n","    for t in text:\n","        entertainment_distances = []\n","        technology_distances = []\n","        medical_distances = []\n","        business_distances = []\n","        token = nltk.word_tokenize(t)\n","        for k in token:\n","            entertainment_distances.append(get_distance(k, 'entertainment', embeddings, word_id, id_word))\n","            technology_distances.append(get_distance(k, 'technology', embeddings, word_id, id_word))\n","            medical_distances.append(get_distance(k, 'medical', embeddings, word_id, id_word))\n","            business_distances.append(get_distance(k, 'business', embeddings, word_id, id_word))\n","        all_embedding_feat.append([min(entertainment_distances), min(technology_distances), min(medical_distances), min(business_distances)])\n","    return all_embedding_feat\n","\n","embed_feat = embedding_extraction(clean_title)\n","embed_feat[:3]"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[2.913120148087336, 2.948760506100634, 3.5125143573508284, 3.091267832678769],\n"," [2.4571364339319977,\n","  3.0249680383428195,\n","  3.3134951266334514,\n","  2.932971141591275],\n"," [3.041295033226138,\n","  3.5203397205450035,\n","  3.7982305924660396,\n","  3.269813735043243]]"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"8lf3sjM4sLcl","executionInfo":{"status":"ok","timestamp":1602150560250,"user_tz":-480,"elapsed":30125,"user":{"displayName":"Hadi Syah Putra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFq8cRfVCtoWipwZWKSOI_noyvowyse5zRadkS2g=s64","userId":"02288041668049318883"}},"outputId":"5837d538-c737-4c4a-b3e8-c7df760a074c","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["'''\n","Function to extract orthography and url occurence features\n","'''\n","def ortography(text):\n","    all_orto_feat = []\n","    for t in text:\n","        capital_count = sum(1 for c in t if c.isupper())\n","        exclamation_count = sum(1 for c in t if c == \"!\")\n","        punctuation_count = sum(1 for c in t if c in punctuation)\n","        word_len = len(nltk.word_tokenize(t))\n","        char_len = len(t)\n","        digit_occurence = sum(1 for c in t if c in digit)\n","        orto_feat = [capital_count, exclamation_count, punctuation_count, word_len, char_len, digit_occurence]\n","        all_orto_feat.append(orto_feat)\n","    return all_orto_feat\n","\n","orto_feat = ortography(df_data['News Title'])\n","orto_feat[:3]"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[2, 0, 3, 10, 58, 0], [4, 0, 1, 5, 31, 0], [7, 0, 0, 10, 48, 0]]"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-10-06T12:07:17.530517Z","start_time":"2020-10-06T12:07:16.746239Z"},"code_folding":[],"id":"cdCIowsf5DVF","executionInfo":{"status":"ok","timestamp":1602150562437,"user_tz":-480,"elapsed":30639,"user":{"displayName":"Hadi Syah Putra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFq8cRfVCtoWipwZWKSOI_noyvowyse5zRadkS2g=s64","userId":"02288041668049318883"}},"outputId":"be89598a-1a6f-456a-a0e0-3495bb7fb77e","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["'''\n","Function to extract TF-IDF (1-gram) features\n","'''\n","def tf_idf_extraction(text):\n","    vectorizer = TfidfVectorizer(max_features=3000)\n","    tfidf_matrix = vectorizer.fit_transform(np.array(text)).todense()\n","    return tfidf_matrix\n","\n","tfidf_feat = tf_idf_extraction(clean_title)\n","print(tfidf_feat[:3])"],"execution_count":23,"outputs":[{"output_type":"stream","text":["[[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9cLWq0bu5DVH"},"source":["# Modelling"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-10-06T12:07:20.299121Z","start_time":"2020-10-06T12:07:20.290299Z"},"id":"ZGthldne5DVI","executionInfo":{"status":"ok","timestamp":1602150562438,"user_tz":-480,"elapsed":28948,"user":{"displayName":"Hadi Syah Putra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFq8cRfVCtoWipwZWKSOI_noyvowyse5zRadkS2g=s64","userId":"02288041668049318883"}}},"source":["category = df_data['Category'].astype('category').cat.codes # target variable"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jtBronk85DVK"},"source":["## No sampling"]},{"cell_type":"code","metadata":{"ExecuteTime":{"start_time":"2020-10-06T12:21:38.160Z"},"id":"FG3wO81b5DVL","executionInfo":{"status":"ok","timestamp":1602150696142,"user_tz":-480,"elapsed":161449,"user":{"displayName":"Hadi Syah Putra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFq8cRfVCtoWipwZWKSOI_noyvowyse5zRadkS2g=s64","userId":"02288041668049318883"}},"outputId":"bef3efc6-04fa-4b0c-dce0-026c8f11f177","colab":{"base_uri":"https://localhost:8080/","height":901}},"source":["# list of features combinations\n","feat_list = [ngram_feat, tfidf_feat, np.hstack((ngram_feat, orto_feat)), np.hstack((tfidf_feat, orto_feat))]\n","feat_name = ['tf', 'tf-idf', 'tf and orthography', 'tf-idf and orthography']\n","mnb = MultinomialNB()\n","bnb = BernoulliNB()\n","\n","# list of model to do prediction\n","model_list = [mnb, bnb]\n","model_name = ['Multinomial Naive Bayes', 'Bernoulli Naive Bayes']\n","\n","# build the model and evaluate the performance of it for each feature combination\n","df_recap = pd.DataFrame()\n","for f, fn in zip(feat_list, feat_name):\n","    print(\"Features : \", fn)\n","    X = f\n","    y = category\n","    for m, n in zip(model_list, model_name):\n","        scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro']\n","        scores = cross_validate(m, X, y, cv=4, scoring=scoring)\n","        acc = np.mean(scores['test_accuracy'])\n","        f1 = np.mean(scores['test_f1_macro'])\n","        precision = np.mean(scores['test_precision_macro'])\n","        recall = np.mean(scores['test_recall_macro'])\n","        print(\"Classifier : \", n)\n","        print(\"Accuracy:\", acc)\n","        print(\"F1-Measure:\", f1)\n","        print(\"Precision:\", precision)\n","        print(\"Recall:\", recall)\n","        df_recap = df_recap.append({\n","            'features': fn,\n","            'classifier': n,\n","            'accuracy': acc,\n","            'f1_score': f1,\n","            'precision': precision,\n","            'recall': recall\n","        }, ignore_index=True)\n","        print('='*90)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Features :  tf\n","Classifier :  Multinomial Naive Bayes\n","Accuracy: 0.8751353778616744\n","F1-Measure: 0.8628577423517834\n","Precision: 0.8642987747187635\n","Recall: 0.8615163807126072\n","==========================================================================================\n","Classifier :  Bernoulli Naive Bayes\n","Accuracy: 0.8751811588857591\n","F1-Measure: 0.8628314919583011\n","Precision: 0.8643751922912077\n","Recall: 0.8613813782894787\n","==========================================================================================\n","Features :  tf-idf\n","Classifier :  Multinomial Naive Bayes\n","Accuracy: 0.8754405685449952\n","F1-Measure: 0.8623747470764094\n","Precision: 0.8772442622452288\n","Recall: 0.8511116434872823\n","==========================================================================================\n","Classifier :  Bernoulli Naive Bayes\n","Accuracy: 0.8751811588857591\n","F1-Measure: 0.8628314919583011\n","Precision: 0.8643751922912077\n","Recall: 0.8613813782894787\n","==========================================================================================\n","Features :  tf and orthography\n","Classifier :  Multinomial Naive Bayes\n","Accuracy: 0.8734874174663714\n","F1-Measure: 0.8611321331499312\n","Precision: 0.8609767693372048\n","Recall: 0.8613492074479783\n","==========================================================================================\n","Classifier :  Bernoulli Naive Bayes\n","Accuracy: 0.8751201134843354\n","F1-Measure: 0.8628760845503605\n","Precision: 0.8637052242842962\n","Recall: 0.8621034167495661\n","==========================================================================================\n","Features :  tf-idf and orthography\n","Classifier :  Multinomial Naive Bayes\n","Accuracy: 0.8662851675085145\n","F1-Measure: 0.8518449667691846\n","Precision: 0.8619372117764434\n","Recall: 0.8436426009110891\n","==========================================================================================\n","Classifier :  Bernoulli Naive Bayes\n","Accuracy: 0.8751201134843354\n","F1-Measure: 0.8628760845503605\n","Precision: 0.8637052242842962\n","Recall: 0.8621034167495661\n","==========================================================================================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9wNpydLwoO5m","executionInfo":{"status":"ok","timestamp":1602150697750,"user_tz":-480,"elapsed":1549,"user":{"displayName":"Hadi Syah Putra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFq8cRfVCtoWipwZWKSOI_noyvowyse5zRadkS2g=s64","userId":"02288041668049318883"}},"outputId":"7250161f-fdc0-4d66-a83b-7c475a8a4852","colab":{"base_uri":"https://localhost:8080/","height":297}},"source":["# the recap of scenarios with no sampling\n","df_recap"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>accuracy</th>\n","      <th>classifier</th>\n","      <th>f1_score</th>\n","      <th>features</th>\n","      <th>precision</th>\n","      <th>recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.875135</td>\n","      <td>Multinomial Naive Bayes</td>\n","      <td>0.862858</td>\n","      <td>tf</td>\n","      <td>0.864299</td>\n","      <td>0.861516</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.875181</td>\n","      <td>Bernoulli Naive Bayes</td>\n","      <td>0.862831</td>\n","      <td>tf</td>\n","      <td>0.864375</td>\n","      <td>0.861381</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.875441</td>\n","      <td>Multinomial Naive Bayes</td>\n","      <td>0.862375</td>\n","      <td>tf-idf</td>\n","      <td>0.877244</td>\n","      <td>0.851112</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.875181</td>\n","      <td>Bernoulli Naive Bayes</td>\n","      <td>0.862831</td>\n","      <td>tf-idf</td>\n","      <td>0.864375</td>\n","      <td>0.861381</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.873487</td>\n","      <td>Multinomial Naive Bayes</td>\n","      <td>0.861132</td>\n","      <td>tf and orthography</td>\n","      <td>0.860977</td>\n","      <td>0.861349</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.875120</td>\n","      <td>Bernoulli Naive Bayes</td>\n","      <td>0.862876</td>\n","      <td>tf and orthography</td>\n","      <td>0.863705</td>\n","      <td>0.862103</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.866285</td>\n","      <td>Multinomial Naive Bayes</td>\n","      <td>0.851845</td>\n","      <td>tf-idf and orthography</td>\n","      <td>0.861937</td>\n","      <td>0.843643</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0.875120</td>\n","      <td>Bernoulli Naive Bayes</td>\n","      <td>0.862876</td>\n","      <td>tf-idf and orthography</td>\n","      <td>0.863705</td>\n","      <td>0.862103</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   accuracy               classifier  f1_score                features  \\\n","0  0.875135  Multinomial Naive Bayes  0.862858                      tf   \n","1  0.875181    Bernoulli Naive Bayes  0.862831                      tf   \n","2  0.875441  Multinomial Naive Bayes  0.862375                  tf-idf   \n","3  0.875181    Bernoulli Naive Bayes  0.862831                  tf-idf   \n","4  0.873487  Multinomial Naive Bayes  0.861132      tf and orthography   \n","5  0.875120    Bernoulli Naive Bayes  0.862876      tf and orthography   \n","6  0.866285  Multinomial Naive Bayes  0.851845  tf-idf and orthography   \n","7  0.875120    Bernoulli Naive Bayes  0.862876  tf-idf and orthography   \n","\n","   precision    recall  \n","0   0.864299  0.861516  \n","1   0.864375  0.861381  \n","2   0.877244  0.851112  \n","3   0.864375  0.861381  \n","4   0.860977  0.861349  \n","5   0.863705  0.862103  \n","6   0.861937  0.843643  \n","7   0.863705  0.862103  "]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"r11LQvpD5DVN"},"source":["## Undersampling"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-10-06T12:16:34.796338Z","start_time":"2020-10-06T12:16:30.462395Z"},"id":"mHNxUi3h5DVO","executionInfo":{"status":"ok","timestamp":1602150763830,"user_tz":-480,"elapsed":67618,"user":{"displayName":"Hadi Syah Putra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFq8cRfVCtoWipwZWKSOI_noyvowyse5zRadkS2g=s64","userId":"02288041668049318883"}},"outputId":"2a1158c2-3e03-4be1-b6d8-a91401a2d036","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# list of features combinations\n","feat_list = [ngram_feat, tfidf_feat, np.hstack((ngram_feat, orto_feat)), np.hstack((tfidf_feat, orto_feat))]\n","feat_name = ['tf', 'tf-idf', 'tf and orthography', 'tf-idf and orthography']\n","mnb = MultinomialNB()\n","bnb = BernoulliNB()\n","\n","# list of model to do prediction\n","model_list = [mnb, bnb]\n","model_name = ['Multinomial Naive Bayes', 'Bernoulli Naive Bayes']\n","\n","# build the model and evaluate the performance of it for each feature combination\n","df_recap_undersample = pd.DataFrame()\n","for f, fn in zip(feat_list, feat_name):\n","    print(\"Features : \", fn)\n","    X = f\n","    y = category\n","    under = RandomUnderSampler(random_state=0)\n","    Xt, yt = under.fit_resample(X, y)\n","    display(Counter(yt))\n","    for m, n in zip(model_list, model_name):\n","        scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro']\n","        scores=cross_validate(m, Xt, yt, cv=4, scoring=scoring)\n","        acc=np.mean(scores['test_accuracy'])\n","        f1=np.mean(scores['test_f1_macro'])\n","        precision=np.mean(scores['test_precision_macro'])\n","        recall=np.mean(scores['test_recall_macro'])\n","        print(\"Classifier : \", n)\n","        print(\"Accuracy:\", acc)\n","        print(\"F1-Measure:\", f1)\n","        print(\"Precision:\", precision)\n","        print(\"Recall:\", recall)\n","        df_recap_undersample = df_recap_undersample.append({\n","            'features': fn,\n","            'classifier': n,\n","            'accuracy': acc,\n","            'f1_score': f1,\n","            'precision': precision,\n","            'recall': recall\n","        }, ignore_index=True)\n","        print('='*90)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Features :  tf\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/plain":["Counter({0: 7091, 1: 7091, 2: 7091, 3: 7091})"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Classifier :  Multinomial Naive Bayes\n","Accuracy: 0.8616908757580031\n","F1-Measure: 0.8615720877640367\n","Precision: 0.8615559335230331\n","Recall: 0.8616913996503865\n","==========================================================================================\n","Classifier :  Bernoulli Naive Bayes\n","Accuracy: 0.8629953462135102\n","F1-Measure: 0.8629030755273075\n","Precision: 0.8629031418474231\n","Recall: 0.8629957060637428\n","==========================================================================================\n","Features :  tf-idf\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/plain":["Counter({0: 7091, 1: 7091, 2: 7091, 3: 7091})"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Classifier :  Multinomial Naive Bayes\n","Accuracy: 0.8632068819630518\n","F1-Measure: 0.8631446329461858\n","Precision: 0.8631965030340557\n","Recall: 0.8632076894259135\n","==========================================================================================\n","Classifier :  Bernoulli Naive Bayes\n","Accuracy: 0.8629953462135102\n","F1-Measure: 0.8629030755273075\n","Precision: 0.8629031418474231\n","Recall: 0.8629957060637428\n","==========================================================================================\n","Features :  tf and orthography\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/plain":["Counter({0: 7091, 1: 7091, 2: 7091, 3: 7091})"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Classifier :  Multinomial Naive Bayes\n","Accuracy: 0.8596460301791002\n","F1-Measure: 0.8594427630330932\n","Precision: 0.8593855461622486\n","Recall: 0.8596465042161137\n","==========================================================================================\n","Classifier :  Bernoulli Naive Bayes\n","Accuracy: 0.8619376674658017\n","F1-Measure: 0.8618460812444177\n","Precision: 0.8618580658783684\n","Recall: 0.8619381565595801\n","==========================================================================================\n","Features :  tf-idf and orthography\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/plain":["Counter({0: 7091, 1: 7091, 2: 7091, 3: 7091})"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Classifier :  Multinomial Naive Bayes\n","Accuracy: 0.8430757297983359\n","F1-Measure: 0.8427932291157859\n","Precision: 0.842987520848592\n","Recall: 0.843076192899767\n","==========================================================================================\n","Classifier :  Bernoulli Naive Bayes\n","Accuracy: 0.8619376674658017\n","F1-Measure: 0.8618460812444177\n","Precision: 0.8618580658783684\n","Recall: 0.8619381565595801\n","==========================================================================================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Bc3l5uLaoQzv","executionInfo":{"status":"ok","timestamp":1602150763832,"user_tz":-480,"elapsed":67612,"user":{"displayName":"Hadi Syah Putra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFq8cRfVCtoWipwZWKSOI_noyvowyse5zRadkS2g=s64","userId":"02288041668049318883"}},"outputId":"c8413597-b7e8-424a-c570-362ca3625675","colab":{"base_uri":"https://localhost:8080/","height":297}},"source":["# the recap of scenarios with undersampling\n","df_recap_undersample"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>accuracy</th>\n","      <th>classifier</th>\n","      <th>f1_score</th>\n","      <th>features</th>\n","      <th>precision</th>\n","      <th>recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.861691</td>\n","      <td>Multinomial Naive Bayes</td>\n","      <td>0.861572</td>\n","      <td>tf</td>\n","      <td>0.861556</td>\n","      <td>0.861691</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.862995</td>\n","      <td>Bernoulli Naive Bayes</td>\n","      <td>0.862903</td>\n","      <td>tf</td>\n","      <td>0.862903</td>\n","      <td>0.862996</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.863207</td>\n","      <td>Multinomial Naive Bayes</td>\n","      <td>0.863145</td>\n","      <td>tf-idf</td>\n","      <td>0.863197</td>\n","      <td>0.863208</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.862995</td>\n","      <td>Bernoulli Naive Bayes</td>\n","      <td>0.862903</td>\n","      <td>tf-idf</td>\n","      <td>0.862903</td>\n","      <td>0.862996</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.859646</td>\n","      <td>Multinomial Naive Bayes</td>\n","      <td>0.859443</td>\n","      <td>tf and orthography</td>\n","      <td>0.859386</td>\n","      <td>0.859647</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.861938</td>\n","      <td>Bernoulli Naive Bayes</td>\n","      <td>0.861846</td>\n","      <td>tf and orthography</td>\n","      <td>0.861858</td>\n","      <td>0.861938</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.843076</td>\n","      <td>Multinomial Naive Bayes</td>\n","      <td>0.842793</td>\n","      <td>tf-idf and orthography</td>\n","      <td>0.842988</td>\n","      <td>0.843076</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0.861938</td>\n","      <td>Bernoulli Naive Bayes</td>\n","      <td>0.861846</td>\n","      <td>tf-idf and orthography</td>\n","      <td>0.861858</td>\n","      <td>0.861938</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   accuracy               classifier  f1_score                features  \\\n","0  0.861691  Multinomial Naive Bayes  0.861572                      tf   \n","1  0.862995    Bernoulli Naive Bayes  0.862903                      tf   \n","2  0.863207  Multinomial Naive Bayes  0.863145                  tf-idf   \n","3  0.862995    Bernoulli Naive Bayes  0.862903                  tf-idf   \n","4  0.859646  Multinomial Naive Bayes  0.859443      tf and orthography   \n","5  0.861938    Bernoulli Naive Bayes  0.861846      tf and orthography   \n","6  0.843076  Multinomial Naive Bayes  0.842793  tf-idf and orthography   \n","7  0.861938    Bernoulli Naive Bayes  0.861846  tf-idf and orthography   \n","\n","   precision    recall  \n","0   0.861556  0.861691  \n","1   0.862903  0.862996  \n","2   0.863197  0.863208  \n","3   0.862903  0.862996  \n","4   0.859386  0.859647  \n","5   0.861858  0.861938  \n","6   0.842988  0.843076  \n","7   0.861858  0.861938  "]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"ODHtSwh-5DVQ"},"source":[""],"execution_count":null,"outputs":[]}]}